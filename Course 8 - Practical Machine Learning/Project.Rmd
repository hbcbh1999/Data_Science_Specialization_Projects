---
title: "The Applications of Machine Learning Algorithms on Weight Lifting Exercises"
author: "Yangang Chen"
output: 
  html_document:
    keep_md: true
---

## Abstract

In this report, I classify how well people perform barbell lifts, where class "A" corresponds to the correct performance, and classes "B", "C", "D", "E" are different types of incorrect performances. I use two machine learning algorithms - **Random Forest**, and **Boosting Classfier (e.g. Gradient Boosting Machine)**. First I train these two algorithms on the training set. Then I apply the trained models on the cross validation set and conclude that **the Random Forest algorithm outperforms the Boosting Classfier algorithm**. In the end, I use the trained model to classify the test set, and **the result using the Random Forest algorithm is 95% accurate** (according to the feedback from the Course Project Prediction Quiz).

## Step 1: Loading the data

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, message=FALSE, warning=FALSE, verbose=FALSE, cache=TRUE)
```
```{r}
library (caret)
library (randomForest)
library (knitr)
library (ggplot2)
library (data.table)
```

I download the data for training and testing sets.
```{r, eval=FALSE}
fileUrl <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
download.file(fileUrl, destfile = "training.csv")
fileUrl <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
download.file(fileUrl, destfile = "testing.csv")
```

I notice that the data in both "testing.csv" and "training.csv" contains missing data. In addition, the data in "training.csv" contains special characters "#DIV/0!". Hence, when loading the data, I specify the "na.strings" options and convert all these missing data and special characters into "NA".
```{r}
training <- fread("training.csv", na.strings = c("NA","#DIV/0!"))
testing <- fread("testing.csv", na.strings = "NA")
```

## Step 2: Exploration of data

The following functions help us get some ideas about the data:
```{r, eval=FALSE}
head(training)
names(training)
summary(training)
lapply(training, class)
```

The data contains 160 variables in total. Many of them contains **NA**:
```{r}
nonNA_index <- colSums(is.na(training))==0
names(training[,!nonNA_index,with=FALSE])
```

Some of them are constants:
```{r}
nsv_index <- nearZeroVar(training,saveMetrics=TRUE)
nsv_index <- nsv_index[,"zeroVar"] > 0
names(training[,nsv_index,with=FALSE])
```

## Step 3: Cleaning the data

For convenience, I decide to remove all the variables that contain "NA" and that are constant, as follows:
```{r}
# The following two equivalent expressions subset the data, delete the other columns, and keep "data.table" structure
training <- training[, nonNA_index & !nsv_index, with=FALSE]
#training <- training[, .SD, .SDcols=nonNA_index&!nsv_index]

testing <- testing[, nonNA_index & !nsv_index, with=FALSE]
```

In addition, I notice that the first few variables are irrelavent to how well an activity was performed by the wearer. More specifically, the reference mentioned that relavent quantities are the measurements such as "arm/belt/forearm/dumbbell sensors' orientations". The first few variables have nothing to do with these quantities, such as index (X), wearer's name (user_name), etc. Hence, I remove these variables to avoid overfiitings.
```{r}
# The following expression deletes the selected columns and keeps "data.table" structure
training[,c("V1","user_name","raw_timestamp_part_1","raw_timestamp_part_2",
            "cvtd_timestamp","new_window","num_window"):=NULL]

testing[,c("V1","user_name","raw_timestamp_part_1","raw_timestamp_part_2",
           "cvtd_timestamp","new_window","num_window"):=NULL]
```

After this step, the training and testing data contain 53 variables.

## Step 4: Separating the training data into a training set and a cross validation set

Next, I separate the training data into a training set and a cross validation set, using the standard function "createDataPartition" in the caret package.
```{r}
set.seed(1000)
inTrain <- createDataPartition(training$classe, p=3/4, list=FALSE)
validation <- training[-inTrain,]
training <- training[inTrain,]
```

## Step 5: Principal component analysis

There are still 53 variables in the training/testing set. If the machine learning algorithms are applied directly on these 53 variables, the computational cost will be extremely expensive. Considering this, it is desirable to reduce the dimension of the problem.

The standard technique to achieve this is called "principal component analysis", which uses singular value decomposition (SVD) to reduce the dimension of the problem. I choose the threashold to be 0.9, which means that the number of principal components capture 90% of the variance.
```{r}
index <- grep("classe", colnames(training))
preProc <- preProcess(training[,-index,with=FALSE],method=c("pca","center","scale"),thresh=0.9)
```

Then I apply the PCA to training, cross validation and test sets.
```{r}
trainingPC <- predict(preProc,training[,-index,with=FALSE])
training_data <- data.table(classe=training$classe,trainingPC)

validationPC <- predict(preProc,validation[,-index,with=FALSE])
validation_data <- data.table(classe=validation$classe,validationPC)

testingPC <- predict(preProc,testing[,-index,with=FALSE])
```

## Step 6: Machine Learning Algorithms

### Algorithms I: Random Forest

Random Forest is a highly accurate machine learning algorithm for classification and regression, by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees.

I use the function "train" in the caret package, with the method "rf" (Random Forest), to train the random forest using the training set.
```{r}
modelFit1 <- train(classe~.,method="rf",data=training_data,trControl=trainControl(method="cv"))
```

Then I apply the trained model to the cross validation set
```{r}
predict1 <- predict(modelFit1,validationPC)
```
and analyse the accuracy of the prediction
```{r}
confusionMatrix(validation$classe, predict1)
```
The accuracy of the prediction using the Random Forest algorithm on the cross validation set is 98%, which is quite good.

Eventually I predict the class of the test set by applying the trained model to the test set:
```{r}
result1 <- predict(modelFit1,testingPC)
result1
```
According to the feedback from the Course Project Prediction Quiz, the only mistake occurs on the third test sample. Hence, the prediction is quite accurate.

### Algorithms II: Boosting Classfier (e.g. Gradient Boosting Machine)

Boosting Classfier is a machine learning algorithm for classification and regression, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees.

I use the function "train" in the caret package, with the method "gbm" (Gradient Boosting Machine), to train the random forest using the training set.
```{r,results="hide"}
modelFit2 <- train(classe~.,method="gbm",data=training_data,trControl=trainControl(method="cv"))
```

Then I apply the trained model to the cross validation set
```{r}
predict2 <- predict(modelFit2,validationPC)
```
and analyse the accuracy of the prediction
```{r}
confusionMatrix(validation$classe, predict2)
```
We can see that the accuracy of the prediction using the Boosting Classfier algorithm on the cross validation set is lower than the Random Forest algorithm.

Eventually I predict the class of the test set by applying the trained model to the test set:
```{r}
result2 <- predict(modelFit2,testingPC)
result2
```
Compared to the Random Forest algorithm, the prediction on the test set using the Boosting Classfier algorithm is not as good.
